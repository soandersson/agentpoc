# .env.example
# Copy this file to .env and configure your settings

# Ollama Configuration
# From dev container, use host.docker.internal (Windows/Mac) or 172.17.0.1 (Linux)
OLLAMA_BASE_URL=http://host.docker.internal:11434

# Ollama Model (use a model you have pulled in Ollama)
# Examples: llama3.2, llama3.1, mistral, phi, qwen
OLLAMA_MODEL=llama3.2

# Optional: Enable web search (requires Tavily API key)
ENABLE_WEB_SEARCH=false
TAVILY_API_KEY=your_tavily_api_key_here
